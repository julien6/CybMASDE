{
    "latent_dim": [
        8,
        16,
        32,
        64
    ],
    "n_layers": [
        1,
        3
    ],
    "hidden_dim": [
        64,
        128,
        256,
        512
    ],
    "activation": [
        "relu",
        "tanh",
        "elu"
    ],
    "lr": [
        1e-4,
        1e-2
    ],
    "batch_size": [
        32,
        64,
        128
    ],
    "kl_weight": [
        0.1,
        1.0
    ]
}